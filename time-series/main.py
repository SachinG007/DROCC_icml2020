from __future__ import print_function
import os
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

from collections import OrderedDict
from torch.utils.data import DataLoader, Dataset
import numpy as np
from drocc_trainer import DROCCTrainer

class LSTM_FC(nn.Module):
    """
    Single layer LSTM with a fully connected layer
    on the last hidden state
    """
    def __init__(self,
                 input_dim=32,
                 num_classes=1, 
                 num_hidden_nodes=8
    ):

        super(LSTM_FC, self).__init__()
        self.input_dim = input_dim
        self.num_classes = num_classes
        self.num_hidden_nodes = num_hidden_nodes
        self.encoder = nn.LSTM(input_size=self.input_dim, 
            hidden_size=self.num_hidden_nodes,
            num_layers=1, batch_first=True)
        self.fc = nn.Linear(self.num_hidden_nodes,
            self.num_classes)
        activ = nn.ReLU(True)

    def forward(self, input):
        features = self.encoder(input)[0][:,-1,:]
        # pdb.set_trace()
        logits = self.fc(features)
        return logits

class CustomDataset(Dataset):
    def __init__(self, data, labels):
        self.data = data
        self.labels = labels
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        return torch.from_numpy(self.data[idx]), (self.labels[idx]), torch.tensor([0])

def adjust_learning_rate(epoch, total_epochs, only_ce_epochs, learning_rate, optimizer):
        """Adjust learning rate during training.

        Parameters
        ----------
        epoch: Current training epoch.
        total_epochs: Total number of epochs for training.
        only_ce_epochs: Number of epochs for initial pretraining.
        learning_rate: Initial learning rate for training.
        """
        #We dont want to consider the only ce 
        #based epochs for the lr scheduler
        epoch = epoch - only_ce_epochs
        drocc_epochs = total_epochs - only_ce_epochs
        # lr = learning_rate
        if epoch <= drocc_epochs:
            lr = learning_rate * 0.1
        if epoch <= 0.50 * drocc_epochs:
            lr = learning_rate
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr

        return optimizer

def load_data(path):
    train_data = np.load(os.path.join(path, 'train.npy'), allow_pickle = True)
    print("Train data Shape : ", train_data.shape)
    train_lab = np.ones((train_data.shape[0])) #All positive labelled data points collected
    test_data = np.load(os.path.join(path, 'test_data.npy'), allow_pickle = True)
    test_lab = np.load(os.path.join(path, 'test_labels.npy'), allow_pickle = True)

    ##preprocessing 
    mean=np.mean(train_data,0)
    std=np.std(train_data,0)
    train_data=(train_data-mean)/std

    test_data = (test_data - mean)/std
    train_samples = train_data.shape[0]
    test_samples = test_data.shape[0]
    print("Train Samples: ", train_samples)
    print("Test Samples: ", test_samples)
    train_data = np.expand_dims(train_data, axis =2)
    test_data = np.expand_dims(test_data, axis =2)
    num_features = train_data.shape[2]
    return CustomDataset(train_data, train_lab), CustomDataset(test_data, test_lab), num_features


def main():
    train_dataset, test_dataset, num_features = load_data(args.data_path)
    train_loader = DataLoader(train_dataset, args.batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, args.batch_size, shuffle=True)
    
    model = LSTM_FC(input_dim=1, num_classes=1, num_hidden_nodes=args.hd).to(device)
    if args.optim == 1:
        optimizer = optim.SGD(model.parameters(),
                                  lr=args.lr,
                                  momentum=args.mom)
        print("using SGD")
    else:
        optimizer = optim.Adam(model.parameters(),
                               lr=args.lr)
        print("using Adam")
    
    # Training the model
    trainer = DROCCTrainer(model, optimizer, args.inp_lamda, args.inp_radius, args.gamma, device)
    
    # Restore from checkpoint 
    if args.restore == 1:
        if os.path.exists(os.path.join(args.model_dir, 'model.pt')):
            trainer.load(args.model_dir)
            print("Saved Model Loaded")
    
    trainer.train(train_loader, test_loader, args.lr, adjust_learning_rate, args.epochs,
        metric=args.metric, ascent_step_size=args.ascent_step_size, only_ce_epochs = 0)

    trainer.save(args.model_dir)

if __name__ == '__main__':
    torch.set_printoptions(precision=5)
    
    parser = argparse.ArgumentParser(description='PyTorch Simple Training')
    parser.add_argument('--batch_size', type=int, default=128, metavar='N',
                        help='input batch size for training (default: 128)')
    parser.add_argument('--epochs', type=int, default=100, metavar='N',
                        help='number of epochs to train')
    parser.add_argument('-oce,', '--only_ce_epochs', type=int, default=50, metavar='N',
                        help='number of epochs to train with only CE loss')
    parser.add_argument('--ascent_num_steps', type=int, default=50, metavar='N',
                        help='Number of gradient ascent steps')                        
    parser.add_argument('--hd', type=int, default=128, metavar='N',
                        help='Num hidden nodes')
    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',
                        help='learning rate')
    parser.add_argument('--ascent_step_size', type=float, default=0.001, metavar='LR',
                        help='step size of gradient ascent')                        
    parser.add_argument('--mom', type=float, default=0.99, metavar='M',
                        help='momentum')
    parser.add_argument('--model_dir', default='log',
                        help='directory of model for saving checkpoint')
    parser.add_argument('--one_class_adv', type=int, default=1, metavar='N',
                        help='adv loss to be used or not')
    parser.add_argument('--inp_radius', type=float, default=0.2, metavar='N',
                        help='radius of the ball in input space')
    parser.add_argument('--inp_lamda', type=float, default=1, metavar='N',
                        help='Weight to the adversarial loss for input layer')
    parser.add_argument('--reg', type=float, default=0, metavar='N',
                        help='weight reg')
    parser.add_argument('--restore', type=int, default=1, metavar='N',
                        help='load model ')
    parser.add_argument('--optim', type=int, default=0, metavar='N',
                        help='0 : Adam 1: SGD')
    parser.add_argument('--gamma', type=float, default=2.0, metavar='N',
                        help='r to gamma * r projection')
    parser.add_argument('-d', '--data_path', type=str, default='.')
    parser.add_argument('--metric', type=str, default='F1')
    args = parser. parse_args()

    # settings
    #Checkpoint store path
    model_dir = args.model_dir
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)
    use_cuda = torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")
    
    main()
